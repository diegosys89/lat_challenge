{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latam Challenge Diego Sisalima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente cuaderno tiene como objetivo resolver un challenge propuesto realizando a trav√©s de un enfoque que optimice la memoria y otro enfoque optimizando el tiempo de ejecuci√≥n. \n",
    "\n",
    "## Background\n",
    "\n",
    "La base de datos a analizar consta de una base en texto plano con registros json separados por salto de linea, el tama√±o actual del archivo de aproximadamente 400Mb no presenta un riesgo al tratar de ser analizado en una computadora personal ya que actualmente usualmente estas cuentan con memoria suficiente para realizar el an√°lisis en este tipo de archivos.\n",
    "\n",
    "Siendo estrictamente pragmaticos, har√© uso de mi computador personal para la experimentaci√≥n del challenge por las razones mencionadas y para ahorrar el gasto de recursos innesarios (para no matar moscas a escopetazos como se suele decir), debo sin embargo aclarar que de ser otro escenario donde los tama√±os de los archivos son consirablemente mayores es altamente recomendable tratarlos con otro tipo de herramientas como las que se disponen en la nube como por ejemplo un cluster de Spark con DataProc en GCP o Amazon Glue en AWS que nos permiten distribuir de manera automatica los datos a trav√©s de varios nodos en memoria o en disco, mismo principio que se aplica en este challenge pero a trav√©s de archivos como explicar√© mas adelante dentro de la misma maquina local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo\n",
    "\n",
    "En an√°lisis de datos lo haremos usando la libreria pandas que no permitir√° hacer agrupaciones y las transformaciones necesarias. Al realizar pruebas de ejecuci√≥n se pudo verficar que el cuello de botella en cuanto a tiempo y memoria se concentraba principalmente en la lectura y parseamiento del archivo json, por esta raz√≥n me he concentrado en atacar este cuello de botella al optimizar el codigo en tiempo y en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las librer√≠as a usar se las describe en el siguiente bloque de codigo, a excepci√≥n de la librer√≠a pandas y emoji, el resto de librer√≠as son estandasip  que vienen en el paquete de Python por default.\n",
    "\n",
    "Version de Python: 3.9.12 <br>\n",
    "pandas==1.5.3<br>\n",
    "emoji==2.8.0 <br>\n",
    "\n",
    "Especial menci√≥n a la librer√≠a typing para la validaci√≥n de tipo de datos que tambien viene por defecto en las versiones de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librer√≠as\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el nombre de nuestro archivo y de comandos magic para medir tiempo y memoria\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene\n",
    "por cada uno de esos d√≠as. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha podido notar, el cuello de botella principal es el tiempo de lectura y parseamiento de datos, sin embargo no es necesario parsear todos los campos para solucionar nuestro problema por lo que para ahorrar tiempo de ejecuci√≥n haremos solo un parseo de los campos necesarios leyendo en archivo en bruto y evitando usar el comando de lectura de pandas para json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Lectura del archivo y obteniendo los datos que necesitamos\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    data = []\n",
    "\n",
    "    # Cargamos en memoria las lineas del archivo y vamos analizando linea a linea los campos que necesitamos\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        user = json_value.get(\"user\").get(\"username\")\n",
    "        date = datetime.strptime(json_value.get(\"date\")[:10], \"%Y-%m-%d\").date()\n",
    "        data_id = json_value.get(\"id\")\n",
    "        data.append({\"date\":date, \"user\":user, \"id\":data_id})\n",
    "    \n",
    "    tweets_data = pd.DataFrame(data)\n",
    "    \n",
    "    # Aqui vamos a agrupar para obtener el top 10 days\n",
    "    top_10_days = tweets_data.groupby([\"date\"]).count()\n",
    "    top_10_days = top_10_days.sort_values(\"id\", ascending=False).head(10)\n",
    "    top_10_days = list(top_10_days.index)\n",
    "    \n",
    "    # Filtramos los datos para el top 10 de dias\n",
    "    tweets_data = tweets_data.loc[tweets_data[\"date\"].isin(top_10_days)]\n",
    "\n",
    "    # Agrupamos por el user y tomamos el de mayor numero de tweets por dia\n",
    "    tweets_data = tweets_data.groupby([\"date\",\"user\"]).count()\n",
    "    tweets_data = tweets_data.sort_values([\"date\",\"id\"], ascending=False)\n",
    "    tweets_data = tweets_data.reset_index().groupby(\"date\").first()\n",
    "    \n",
    "    # Transformamos en lista de tuplas para cumplir el tipo de dato de salida\n",
    "    tweets_data = [tuple(i) for i in tweets_data[[\"user\"]].itertuples()]\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Respuesta de la Q1 optimizando tiempo\n",
    "q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 s ¬± 3.76 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1449.68 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos como para nuestro q1 se obtuvo un tiempo de ejecuci√≥n promedio de 18.2 segundos y un uso de memoria de alrededor de 1449.68 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el escenario de optimizaci√≥n de memoria utilizaremos el algoritmo de dividir y vencer. En el caso de la computaci√≥n distruida como en clusteres spark como se mencion√≥ antes se puede controlar esto a trav√©s del particionamiento de la inforaci√≥n, esto permitir√° a que cada nodo maneje un rango de datos mas peque√±o, sin embargo para esto en fundamental el escalamiento horizontal, que en otras palabras significa agregar mas nodos al cluster de procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, y que de igual forma aplicaremos a las funciones q2 y q3 de optimizaci√≥n de memoria, ser√° trabajar con archivos, que nos servir√°n a manera de \"nodos\" para hacer un procesamiento previo. Dicho de otra manera, ya que queremos obtener un resumen por dia del numero de tweets y por usuario, leeremos linea a linea el archivo de origin y lo iremos distribuyendo en varios archivos, uno por cada fecha a partir de los cuales calcularemos los valores deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path):\n",
    "    # Se leer√° el archivo de datos origin y linea por linea se distibuye a un archivo de acuerdo a su fecha\n",
    "    # A su vez, contaremos el numero de lineas que tiene cada archivo para asi obterner el numero de tweets por cada dia\n",
    "    file_lines = {}\n",
    "\n",
    "    # Hacemos la lectura del archivo linea a linea\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_value = json.loads(line)\n",
    "            date = json_value.get(\"date\")[:10]\n",
    "            user = json_value.get(\"user\").get(\"username\")\n",
    "            data_id = json_value.get(\"id\")\n",
    "            data = {\"user\":user, \"id\":data_id}\n",
    "            with open(f\"data_q1/{date}\",\"a\") as fwrite:\n",
    "                fwrite.write(json.dumps(data)+\"\\n\") # Guardamos el registro en su archivo correspondiente\n",
    "                try:\n",
    "                    file_lines[date] += 1 # Contamos el numero de lineas en un diccionario por dia\n",
    "                except:\n",
    "                    file_lines[date] = 0 # Si no existe se empieza a contar\n",
    "    \n",
    "    # Convertimos en dataframe a nuestro diccionario y obtenemos el top 10 de dias con mas tweets\n",
    "    top_10_days = pd.DataFrame([{\"date\":i, \"rows\":file_lines[i]} for i in file_lines])\n",
    "    top_10_days = top_10_days.sort_values(\"rows\", ascending = False)[:10]\n",
    "    top_10_days = list(top_10_days['date'])\n",
    "    \n",
    "    # Tendremos asi particionados nuestros datos, leeremos ahora si con pandas read json y agrupamos por usuario para tomar el primero\n",
    "    result_list = []\n",
    "    # Notese que solo leemos los archivos de los top 10 de d√≠as\n",
    "    for i in top_10_days:\n",
    "        data_tmp = pd.read_json(f'data_q1/{i}', orient='records', lines=True)\n",
    "        data_tmp = data_tmp.groupby(\"user\").count().sort_values(\"id\", ascending = False)[:1]\n",
    "        user = data_tmp.index.values[0]\n",
    "        result_list.append((i, user))\n",
    "    \n",
    "    # Borramos los archivos auxiliares generados para alguna proxima ejecuci√≥n\n",
    "    directory_path = 'data_q1'\n",
    "    file_list = os.listdir(directory_path)\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021-02-12', 'RanbirS00614606'),\n",
       " ('2021-02-13', 'MaanDee08215437'),\n",
       " ('2021-02-17', 'RaaJVinderkaur'),\n",
       " ('2021-02-16', 'jot__b'),\n",
       " ('2021-02-14', 'rebelpacifist'),\n",
       " ('2021-02-18', 'neetuanjle_nitu'),\n",
       " ('2021-02-15', 'jot__b'),\n",
       " ('2021-02-20', 'MangalJ23056160'),\n",
       " ('2021-02-23', 'Surrypuria'),\n",
       " ('2021-02-19', 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 3s ¬± 11.6 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 155.22 MiB, increment: 2.24 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenidos una vez los datos comprobamos como la ejecuci√≥n q1_memory hizo un descenco significado\n",
    "\n",
    "| Funcion | Tiempo | Memoria |\n",
    "|---------|--------|---------|\n",
    "| q1_time | 18.2s | 1449.68 Mib |\n",
    "| q1_memory | 1min 3s | 155.22 MiB |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Los top 10 emojis m√°s usados con su respectivo conteo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera para el conteo de emojis usaremos un enfoque similar, leeremos solos los campos necesarios en la lectura del archivo, en este caso el campo content, de cual obtendremos los emojis con la ayuda de la funci√≥n analiza de la libreria emoji. Extraemos en una lista y la concatenamos en una lista global para todos el archivo para finalmente agruparlos usando un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # En este caso haremos la lectura igual que en la primera pregunta pero extraeremos unicamente el campo content\n",
    "    file_data = open(file_path, 'r')\n",
    "    Lines = file_data.readlines()\n",
    "    data = []\n",
    "\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        data.append(json_value.get(\"content\"))\n",
    "\n",
    "    # Lista donde iremos concatenando los emojis de todo el archivo\n",
    "    emoji_values = []\n",
    "    \n",
    "    for i in data:\n",
    "        # Emoji analize retorna un tipo de dato con el valor del emoji y su posici√≥n, en este caso solo tomaremos el valor\n",
    "        emoji_values += [value.chars for value in emoji.analyze(i)]\n",
    "\n",
    "    #Vamos a crear un dataframe con los resultados\n",
    "    data = pd.DataFrame({\"emoji\":emoji_values})\n",
    "    data[\"counter\"] = 1\n",
    "    data = data.groupby('emoji').sum().sort_values(\"counter\", ascending = False).head(10)\n",
    "    emoji_list = [tuple(i) for i in data[[\"counter\"]].itertuples()]\n",
    "    file_data.close() #Liberamos memoria\n",
    "    \n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 5049),\n",
       " ('üòÇ', 3072),\n",
       " ('üöú', 2972),\n",
       " ('üåæ', 2182),\n",
       " ('üáÆüá≥', 2086),\n",
       " ('ü§£', 1668),\n",
       " ('‚úä', 1651),\n",
       " ('‚ù§Ô∏è', 1382),\n",
       " ('üôèüèª', 1317),\n",
       " ('üíö', 1040)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.5 s ¬± 2.83 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 597.83 MiB, increment: 16.88 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit resultado_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la ejecuci√≥n con optimizaci√≥n de memoria nos apoyaremos de archivos en local para hacer un paso intermedio en el procesamiento, extraermos linea a linea los emojis de nuestra fuente de datos y los guardaremos en un archivo en local.\n",
    "\n",
    "Para el agrupamiento iremos leyendo linea a linea el archivo de preprocesamiento y los iremos agrupando en un diccionario de datos el cual procesaremos con pandas para obetener la respuesta deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Para evitar consumir la memoria en la lectura del archivo iremos leyendo linea a linea el archivo\n",
    "    # haremos una cuenta un agrupamiento previo e iremos almacenando en un archivo\n",
    "\n",
    "    # Lectura linea a linea\n",
    "    file1 = open(file_path,\"r\")\n",
    "    fwrite = open(\"aux_mem_q2/emoji_data\",\"a\")\n",
    "    file_read = open(\"aux_mem_q2/emoji_data\",\"r\")\n",
    "\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        # Leeremos solo el valor del content donde estan los emojis\n",
    "        content = json_value.get(\"content\")\n",
    "        \n",
    "        # Extraemos emojis y los agrupamos\n",
    "        tmp_emoji_list = [value.chars for value in emoji.analyze(content)]\n",
    "        tmp_emoji_list = '\\n'.join(tmp_emoji_list)\n",
    "        \n",
    "        # Si existen emojis en la linea los guardaremos en el archivo previo\n",
    "        if tmp_emoji_list:\n",
    "            fwrite.write(tmp_emoji_list+\"\\n\")\n",
    "\n",
    "    file1.close()\n",
    "    fwrite.close()\n",
    "        \n",
    "    # Ahora leeremos el archivo fila a fila y almacenaremos en un dictionario de datos sumando uno por cada ocurrencia\n",
    "    emoji_values = {}\n",
    "\n",
    "    for emoji_line in file_read:\n",
    "        if emoji_line.replace('\\n','') in emoji_values.keys():\n",
    "            emoji_values[emoji_line.replace('\\n','')] +=1 # Si existe se suma uno\n",
    "        else:\n",
    "            emoji_values[emoji_line.replace('\\n','')] = 0 # Si no existe lo crea\n",
    "    \n",
    "    #Lo hacemos dataframe para agrupar y sacar los maximos\n",
    "    emoji_values = pd.DataFrame({\"emoji\":list(emoji_values.keys()),\"conteo\":list(emoji_values.values())})\n",
    "    emoji_values = emoji_values.sort_values('conteo', ascending=False).head(10)\n",
    "\n",
    "    # #Lo volvemos listado de tuplas\n",
    "    emoji_values = [tuple(i) for i in emoji_values.set_index('emoji').itertuples()]\n",
    "\n",
    "    # Borrar el archivo generado\n",
    "    os.remove('aux_mem_q2/emoji_data')\n",
    "    file1.close()\n",
    "    \n",
    "    return emoji_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 5048),\n",
       " ('üòÇ', 3071),\n",
       " ('üöú', 2971),\n",
       " ('üåæ', 2181),\n",
       " ('üáÆüá≥', 2085),\n",
       " ('ü§£', 1667),\n",
       " ('‚úä', 1650),\n",
       " ('‚ù§Ô∏è', 1381),\n",
       " ('üôèüèª', 1316),\n",
       " ('üíö', 1039)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.3 s ¬± 7.45 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 158.62 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera si hacemos una tabla comparativa podemos observar de mejor manera como efectivamente se aplico la optmizaci√≥n de memoria y tiempo respectivamente\n",
    "\n",
    "\n",
    "| Funcion | Tiempo | Memoria |\n",
    "|---------|--------|---------|\n",
    "| q2_time | 34.5s | 597.83.68 Mib |\n",
    "| q2_memory | 40.3s | 158.62 MiB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este problema, se tomar√° de igual manera los campos estrictamente necesarios, en este caso mentionedUsers el cual contiene un listado de usernames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # La documentacion no lo especifica pero en un analsis del archivo se puede observar que existe el campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre as√≠\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "    \n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.73 s ¬± 1.13 s per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 581.56 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la optimizaci√≥n de memoria de crearemos una lista donde se iran almancenando los valores linea a linea de las menciones de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # Se toma el mismo campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    #La lectura es linea a linea para no saturar memoria\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre as√≠\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "\n",
    "    file1.close()\n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 s ¬± 960 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 150.88 MiB, increment: 6.93 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabla comparativa de tiempo de ejecuci√≥n y uso de memoria por cada funci√≥n\n",
    "\n",
    "\n",
    "| Funcion | Tiempo | Memoria |\n",
    "|---------|--------|---------|\n",
    "| q3_time | 6.73 | 581.56.68 Mib |\n",
    "| q3_memory | 11.1s  | 150.88 MiB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Se ha logrado hacer una optimizaci√≥n de las ejecuciones, al indentificar el cuello de botella que es la lectura y parseo de la informaci√≥n hemos podido atacar este paso en las ejecuciones dependiendo del tipo de optimizaci√≥n que se deseaba.\n",
    "\n",
    "Aunque nuestra implementaci√≥n es una para una maquina unica, hemos aplicado algunos principios b√°sicos que se hacen uso en la computaci√≥n distribuida como es el particionamiento y el procesamiento en lotes.\n",
    "\n",
    "En caso de requerir procesar archivos mas grandes debemos hacer uso de cluster de procesamiento como Spark para distribuir de manera optima los datos, nuestros codigos pueden ser facilmente replanteados para DataFrames PySpark o usando Spark SQL para el procesamiento\n",
    "\n",
    "Me gustar√≠a realizar una observaci√≥n para el challenge, el esquema de la data presenta actualmente se encuentra desactualizada con respecto a la documentaci√≥n oficial por lo que recomiendo la actualizaci√≥n de la data del challenge a uno con un esquema actual."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445f001600bf7eb0ddc22245e91c6123825f79238ec2a014331a6132a9c2d200"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
