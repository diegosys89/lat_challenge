{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de base de datos con pandas\n",
    "\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la primera version se puede notar claramente que el cuello de botella corresponde a la lectura y parseo del archivo Json, en donde incluso se observa que el alrededor del 92% del tiempo de uso llega a ser por el tiempo de lectura del archivo json\n",
    "\n",
    "Para la lectura del archivo Json se crearÃ¡ en este caso una funciÃ³n que optimice el tiempo de lectura del archivo json y retornado solo los campos deseados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las top 10 fechas donde hay mÃ¡s tweets. Mencionar el usuario (username) que mÃ¡s publicaciones tiene\n",
    "por cada uno de esos dÃ­as. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Lectura del archivo y obteniendo los datos que necesitamos\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    data = []\n",
    "\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        user = json_value.get(\"user\").get(\"username\")\n",
    "        date = datetime.strptime(json_value.get(\"date\")[:10], \"%Y-%m-%d\").date()\n",
    "        data_id = json_value.get(\"id\")\n",
    "        data.append({\"date\":date, \"user\":user, \"id\":data_id})\n",
    "    \n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    #top 10 days\n",
    "    top_10_days = tweets_data.groupby([\"date\"]).count()\n",
    "    top_10_days = top_10_days.sort_values(\"id\", ascending=False).head(10)\n",
    "    top_10_days = list(top_10_days.index)\n",
    "    \n",
    "    #filter data and group by date and user\n",
    "    tweets_data = tweets_data.loc[tweets_data[\"date\"].isin(top_10_days)]\n",
    "    tweets_data = tweets_data.groupby([\"date\",\"user\"]).count()\n",
    "\n",
    "    tweets_data = tweets_data.sort_values([\"date\",\"id\"], ascending=False)\n",
    "    tweets_data = tweets_data.reset_index().groupby(\"date\").first()\n",
    "    \n",
    "    tweets_data = [tuple(i) for i in tweets_data[[\"user\"]].itertuples()]\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3 s Â± 1.27 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en este caso el valor de lectura de tiempo se reduce considerablemente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de optimizacion de memoria utilizaremos archivos en disco que almacenen datos de manera temporal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dividirÃ¡ la lectura en varios lotes, aplicando una tecnica de map reduce y guardando los archivos con los resultados previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path):\n",
    "    # Create dict of date values and num of lines in file\n",
    "    file_lines = {}\n",
    "\n",
    "    # Open the file for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_value = json.loads(line)\n",
    "            date = json_value.get(\"date\")[:10]\n",
    "            user = json_value.get(\"user\").get(\"username\")\n",
    "            data_id = json_value.get(\"id\")\n",
    "            data = {\"user\":user, \"id\":data_id}\n",
    "            with open(f\"data_q1/{date}\",\"a\") as fwrite:\n",
    "                fwrite.write(json.dumps(data)+\"\\n\")\n",
    "                try:\n",
    "                    file_lines[date] += 1 \n",
    "                except:\n",
    "                    file_lines[date] = 0\n",
    "    \n",
    "    top_10_days = pd.DataFrame([{\"date\":i, \"rows\":file_lines[i]} for i in file_lines])\n",
    "    top_10_days = top_10_days.sort_values(\"rows\", ascending = False)[:10]\n",
    "    top_10_days = list(top_10_days['date'])\n",
    "    \n",
    "    result_list = []\n",
    "    for i in top_10_days:\n",
    "        data_tmp = pd.read_json(f'data_q1/{i}', orient='records', lines=True)\n",
    "        data_tmp = data_tmp.groupby(\"user\").count().sort_values(\"id\", ascending = False)[:1]\n",
    "        user = data_tmp.index.values[0]\n",
    "        result_list.append((i, user))\n",
    "    \n",
    "    # delete all previous files generated\n",
    "    directory_path = 'data_q1'\n",
    "    file_list = os.listdir(directory_path)\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 149.06 MiB, increment: 10.73 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Los top 10 emojis mÃ¡s usados con su respectivo conteo. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #En este caso haremos la lectura igual que en la primera pregunta pero extraeremos unicamente el campo content\n",
    "    file_data = open(file_path, 'r')\n",
    "    Lines = file_data.readlines()\n",
    "    data = []\n",
    "\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        data.append(json_value.get(\"content\"))\n",
    "\n",
    "    emoji_values = []\n",
    "    \n",
    "    for i in data:\n",
    "        emoji_values += [value.chars for value in emoji.analyze(i)]\n",
    "\n",
    "    #Vamos a crear un dataframe con los resultados\n",
    "    data = pd.DataFrame({\"emoji\":emoji_values})\n",
    "    data[\"counter\"] = 1\n",
    "    data = data.groupby('emoji').sum().sort_values(\"counter\", ascending = False).head(10)\n",
    "    emoji_list = [tuple(i) for i in data[[\"counter\"]].itertuples()]\n",
    "    file_data.close() #Liberamos memoria\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 594.25 MiB, increment: 20.25 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit resultado_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.6 s Â± 2.47 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit resultado_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ðŸ™', 5049),\n",
       " ('ðŸ˜‚', 3072),\n",
       " ('ðŸšœ', 2972),\n",
       " ('ðŸŒ¾', 2182),\n",
       " ('ðŸ‡®ðŸ‡³', 2086),\n",
       " ('ðŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ðŸ™ðŸ»', 1317),\n",
       " ('ðŸ’š', 1040)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimir resultado\n",
    "resultado_q2_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Para evitar consumir la memoria en la lectura del archivo iremos leyendo linea a linea el archivo\n",
    "    # haremos una cuenta un agrupamiento previo e iremos almacenando en un archivo\n",
    "\n",
    "    # Lectura linea a linea\n",
    "    file1 = open(file_path,\"r\")\n",
    "    fwrite = open(\"aux_mem_q2/emoji_data\",\"a\")\n",
    "    file_read = open(\"aux_mem_q2/emoji_data\",\"r\")\n",
    "\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        # Leeremos solo el valor del content donde estar los emojis\n",
    "        content = json_value.get(\"content\")\n",
    "        \n",
    "        #Extraemos emojis y los agrupamos\n",
    "        tmp_emoji_list = [value.chars for value in emoji.analyze(content)]\n",
    "        tmp_emoji_list = '\\n'.join(tmp_emoji_list)\n",
    "        \n",
    "        if tmp_emoji_list: #Emojis existen\n",
    "            fwrite.write(tmp_emoji_list+\"\\n\")\n",
    "\n",
    "    file1.close()\n",
    "    fwrite.close()\n",
    "        \n",
    "    # Ahora leeremos el archivo fila a fila y almacenaremos en un dictionario de datos sumando uno por cada ocurrencia\n",
    "    emoji_values = {}\n",
    "\n",
    "    for emoji_line in file_read:\n",
    "        if emoji_line.replace('\\n','') in emoji_values.keys():\n",
    "            emoji_values[emoji_line.replace('\\n','')] +=1 # Si existe se suma uno\n",
    "        else:\n",
    "            emoji_values[emoji_line.replace('\\n','')] = 0 # Si no existe lo crea\n",
    "    \n",
    "    #Lo hacemos dataframe para agrupar y sacar los maximos\n",
    "    emoji_values = pd.DataFrame({\"emoji\":list(emoji_values.keys()),\"conteo\":list(emoji_values.values())})\n",
    "    emoji_values = emoji_values.sort_values('conteo', ascending=False).head(10)\n",
    "\n",
    "    # #Lo volvemos listado de tuplas\n",
    "    emoji_values = [tuple(i) for i in emoji_values.set_index('emoji').itertuples()]\n",
    "\n",
    "    # Borrar el archivo generado\n",
    "    os.remove('aux_mem_q2/emoji_data')\n",
    "    file1.close()\n",
    "    \n",
    "    return emoji_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 158.62 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit x = q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ðŸ™', 15146),\n",
       " ('ðŸ˜‚', 9215),\n",
       " ('ðŸšœ', 8915),\n",
       " ('ðŸŒ¾', 6545),\n",
       " ('ðŸ‡®ðŸ‡³', 6257),\n",
       " ('ðŸ¤£', 5003),\n",
       " ('âœŠ', 4952),\n",
       " ('â¤ï¸', 4145),\n",
       " ('ðŸ™ðŸ»', 3950),\n",
       " ('ðŸ’š', 3119)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. El top 10 histÃ³rico de usuarios (username) mÃ¡s influyentes en funciÃ³n del conteo de las menciones (@)\n",
    "que registra cada uno de ellos. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # La documentacion no lo especifica pero en un analsis del archivo se puede observar que existe el campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre asÃ­\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "    \n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.36 s Â± 590 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 582.40 MiB, increment: 412.10 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # Se toma el mismo campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    #La lectura es linea a linea para no saturar memoria\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre asÃ­\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "\n",
    "    file1.close()\n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 s Â± 960 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 150.88 MiB, increment: 6.93 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(file_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445f001600bf7eb0ddc22245e91c6123825f79238ec2a014331a6132a9c2d200"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
