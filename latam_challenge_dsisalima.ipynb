{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de base de datos con pandas\n",
    "\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la primera version se puede notar claramente que el cuello de botella corresponde a la lectura y parseo del archivo Json, en donde incluso se observa que el alrededor del 92% del tiempo de uso llega a ser por el tiempo de lectura del archivo json\n",
    "\n",
    "Para la lectura del archivo Json se creará en este caso una función que optimice el tiempo de lectura del archivo json y retornado solo los campos deseados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene\n",
    "por cada uno de esos días. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Lectura del archivo y obteniendo los datos que necesitamos\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    data = []\n",
    "\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        user = json_value.get(\"user\").get(\"username\")\n",
    "        date = datetime.strptime(json_value.get(\"date\")[:10], \"%Y-%m-%d\").date()\n",
    "        data_id = json_value.get(\"id\")\n",
    "        data.append({\"date\":date, \"user\":user, \"id\":data_id})\n",
    "    \n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    #top 10 days\n",
    "    top_10_days = tweets_data.groupby([\"date\"]).count()\n",
    "    top_10_days = top_10_days.sort_values(\"id\", ascending=False).head(10)\n",
    "    top_10_days = list(top_10_days.index)\n",
    "    \n",
    "    #filter data and group by date and user\n",
    "    tweets_data = tweets_data.loc[tweets_data[\"date\"].isin(top_10_days)]\n",
    "    tweets_data = tweets_data.groupby([\"date\",\"user\"]).count()\n",
    "\n",
    "    tweets_data = tweets_data.sort_values([\"date\",\"id\"], ascending=False)\n",
    "    tweets_data = tweets_data.reset_index().groupby(\"date\").first()\n",
    "    \n",
    "    tweets_data = [tuple(i) for i in tweets_data[[\"user\"]].itertuples()]\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3 s ± 1.27 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en este caso el valor de lectura de tiempo se reduce considerablemente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de optimizacion de memoria utilizaremos archivos en disco que almacenen datos de manera temporal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dividirá la lectura en varios lotes, aplicando una tecnica de map reduce y guardando los archivos con los resultados previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path):\n",
    "    # Create dict of date values and num of lines in file\n",
    "    file_lines = {}\n",
    "\n",
    "    # Open the file for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_value = json.loads(line)\n",
    "            date = json_value.get(\"date\")[:10]\n",
    "            user = json_value.get(\"user\").get(\"username\")\n",
    "            data_id = json_value.get(\"id\")\n",
    "            data = {\"user\":user, \"id\":data_id}\n",
    "            with open(f\"data_q1/{date}\",\"a\") as fwrite:\n",
    "                fwrite.write(json.dumps(data)+\"\\n\")\n",
    "                try:\n",
    "                    file_lines[date] += 1 \n",
    "                except:\n",
    "                    file_lines[date] = 0\n",
    "    \n",
    "    top_10_days = pd.DataFrame([{\"date\":i, \"rows\":file_lines[i]} for i in file_lines])\n",
    "    top_10_days = top_10_days.sort_values(\"rows\", ascending = False)[:10]\n",
    "    top_10_days = list(top_10_days['date'])\n",
    "    \n",
    "    result_list = []\n",
    "    for i in top_10_days:\n",
    "        data_tmp = pd.read_json(f'data_q1/{i}', orient='records', lines=True)\n",
    "        data_tmp = data_tmp.groupby(\"user\").count().sort_values(\"id\", ascending = False)[:1]\n",
    "        user = data_tmp.index.values[0]\n",
    "        result_list.append((i, user))\n",
    "    \n",
    "    # delete all previous files generated\n",
    "    directory_path = 'data_q1'\n",
    "    file_list = os.listdir(directory_path)\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 149.06 MiB, increment: 10.73 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Los top 10 emojis más usados con su respectivo conteo. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #En este caso haremos la lectura igual que en la primera pregunta pero extraeremos unicamente el campo content\n",
    "    file_data = open(file_path, 'r')\n",
    "    Lines = file_data.readlines()\n",
    "    data = []\n",
    "\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        data.append(json_value.get(\"content\"))\n",
    "\n",
    "    emoji_values = []\n",
    "    \n",
    "    for i in data:\n",
    "        emoji_values += [value.chars for value in emoji.analyze(i)]\n",
    "\n",
    "    #Vamos a crear un dataframe con los resultados\n",
    "    data = pd.DataFrame({\"emoji\":emoji_values})\n",
    "    data[\"counter\"] = 1\n",
    "    data = data.groupby('emoji').sum().sort_values(\"counter\", ascending = False).head(10)\n",
    "    emoji_list = [tuple(i) for i in data[[\"counter\"]].itertuples()]\n",
    "    file_data.close() #Liberamos memoria\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 594.25 MiB, increment: 20.25 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit resultado_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.6 s ± 2.47 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit resultado_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimir resultado\n",
    "resultado_q2_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Para evitar consumir la memoria en la lectura del archivo iremos leyendo linea a linea el archivo\n",
    "    # haremos una cuenta un agrupamiento previo e iremos almacenando en un archivo\n",
    "\n",
    "    # Lectura linea a linea\n",
    "    file1 = open(file_path,\"r\")\n",
    "    fwrite = open(\"aux_mem_q2/emoji_data\",\"a\")\n",
    "    file_read = open(\"aux_mem_q2/emoji_data\",\"r\")\n",
    "\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        # Leeremos solo el valor del content donde estar los emojis\n",
    "        content = json_value.get(\"content\")\n",
    "        \n",
    "        #Extraemos emojis y los agrupamos\n",
    "        tmp_emoji_list = [value.chars for value in emoji.analyze(content)]\n",
    "        tmp_emoji_list = '\\n'.join(tmp_emoji_list)\n",
    "        \n",
    "        if tmp_emoji_list: #Emojis existen\n",
    "            fwrite.write(tmp_emoji_list+\"\\n\")\n",
    "\n",
    "    file1.close()\n",
    "    fwrite.close()\n",
    "        \n",
    "    # Ahora leeremos el archivo fila a fila y almacenaremos en un dictionario de datos sumando uno por cada ocurrencia\n",
    "    emoji_values = {}\n",
    "\n",
    "    for emoji_line in file_read:\n",
    "        if emoji_line.replace('\\n','') in emoji_values.keys():\n",
    "            emoji_values[emoji_line.replace('\\n','')] +=1 # Si existe se suma uno\n",
    "        else:\n",
    "            emoji_values[emoji_line.replace('\\n','')] = 0 # Si no existe lo crea\n",
    "    \n",
    "    #Lo hacemos dataframe para agrupar y sacar los maximos\n",
    "    emoji_values = pd.DataFrame({\"emoji\":list(emoji_values.keys()),\"conteo\":list(emoji_values.values())})\n",
    "    emoji_values = emoji_values.sort_values('conteo', ascending=False).head(10)\n",
    "\n",
    "    # #Lo volvemos listado de tuplas\n",
    "    emoji_values = [tuple(i) for i in emoji_values.set_index('emoji').itertuples()]\n",
    "\n",
    "    # Borrar el archivo generado\n",
    "    os.remove('aux_mem_q2/emoji_data')\n",
    "    file1.close()\n",
    "    \n",
    "    return emoji_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 158.62 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit x = q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 15146),\n",
       " ('😂', 9215),\n",
       " ('🚜', 8915),\n",
       " ('🌾', 6545),\n",
       " ('🇮🇳', 6257),\n",
       " ('🤣', 5003),\n",
       " ('✊', 4952),\n",
       " ('❤️', 4145),\n",
       " ('🙏🏻', 3950),\n",
       " ('💚', 3119)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@)\n",
    "que registra cada uno de ellos. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # La documentacion no lo especifica pero en un analsis del archivo se puede observar que existe el campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre así\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "    \n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.36 s ± 590 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 582.40 MiB, increment: 412.10 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # Se toma el mismo campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    #La lectura es linea a linea para no saturar memoria\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre así\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "\n",
    "    file1.close()\n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 s ± 960 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 150.88 MiB, increment: 6.93 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(file_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445f001600bf7eb0ddc22245e91c6123825f79238ec2a014331a6132a9c2d200"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
