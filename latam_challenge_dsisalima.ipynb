{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latam Challenge Diego Sisalima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente cuaderno tiene como objetivo resolver un challenge propuesto realizando a través de un enfoque que optimice la memoria y otro enfoque optimizando el tiempo de ejecución. \n",
    "\n",
    "## Background\n",
    "\n",
    "La base de datos a analizar consta de una base en texto plano con registros json separados por salto de linea, el tamaño actual del archivo de aproximadamente 400Mb no presenta un riesgo al tratar de ser analizado en una computadora personal ya que actualmente usualmente estas cuentan con memoria suficiente para realizar el análisis en este tipo de archivos.\n",
    "\n",
    "Siendo estrictamente pragmaticos, haré uso de mi computador personal para la experimentación del challenge por las razones mencionadas y para ahorrar el gasto de recursos innesarios (para no matar moscas a escopetazos como se suele decir), debo sin embargo aclarar que de ser otro escenario donde los tamaños de los archivos son consirablemente mayores es altamente recomendable tratarlos con otro tipo de herramientas como las que se disponen en la nube como por ejemplo un cluster de Spark con DataProc en GCP o Amazon Glue en AWS que nos permiten distribuir de manera automatica los datos a través de varios nodos en memoria o en disco, mismo principio que se aplica en este challenge pero a través de archivos como explicaré mas adelante dentro de la misma maquina local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo\n",
    "\n",
    "En análisis de datos lo haremos usando la libreria pandas que no permitirá hacer agrupaciones y las transformaciones necesarias. Al realizar pruebas de ejecución se pudo verficar que el cuello de botella en cuanto a tiempo y memoria se concentraba principalmente en la lectura y parseamiento del archivo json, por esta razón me he concentrado en atacar este cuello de botella al optimizar el codigo en tiempo y en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las librerías a usar se las describe en el siguiente bloque de codigo, a excepción de la librería pandas y emoji, el resto de librerías son estandasip  que vienen en el paquete de Python por default.\n",
    "\n",
    "Version de Python: 3.9.12 <br>\n",
    "pandas==1.5.3<br>\n",
    "emoji==2.8.0 <br>\n",
    "\n",
    "Especial mención a la librería typing para la validación de tipo de datos que tambien viene por defecto en las versiones de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el nombre de nuestro archivo y de comandos magic para medir tiempo y memoria\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene\n",
    "por cada uno de esos días. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha podido notar, el cuello de botella principal es el tiempo de lectura y parseamiento de datos, sin embargo no es necesario parsear todos los campos para solucionar nuestro problema por lo que para ahorrar tiempo de ejecución haremos solo un parseo de los campos necesarios leyendo en archivo en bruto y evitando usar el comando de lectura de pandas para json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    # Lectura del archivo y obteniendo los datos que necesitamos\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    data = []\n",
    "\n",
    "    # Cargamos en memoria las lineas del archivo y vamos analizando linea a linea los campos que necesitamos\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        user = json_value.get(\"user\").get(\"username\")\n",
    "        date = datetime.strptime(json_value.get(\"date\")[:10], \"%Y-%m-%d\").date()\n",
    "        data_id = json_value.get(\"id\")\n",
    "        data.append({\"date\":date, \"user\":user, \"id\":data_id})\n",
    "    \n",
    "    tweets_data = pd.DataFrame(data)\n",
    "    \n",
    "    # Aqui vamos a agrupar para obtener el top 10 days\n",
    "    top_10_days = tweets_data.groupby([\"date\"]).count()\n",
    "    top_10_days = top_10_days.sort_values(\"id\", ascending=False).head(10)\n",
    "    top_10_days = list(top_10_days.index)\n",
    "    \n",
    "    # Filtramos los datos para el top 10 de dias\n",
    "    tweets_data = tweets_data.loc[tweets_data[\"date\"].isin(top_10_days)]\n",
    "\n",
    "    # Agrupamos por el user y tomamos el de mayor numero de tweets por dia\n",
    "    tweets_data = tweets_data.groupby([\"date\",\"user\"]).count()\n",
    "    tweets_data = tweets_data.sort_values([\"date\",\"id\"], ascending=False)\n",
    "    tweets_data = tweets_data.reset_index().groupby(\"date\").first()\n",
    "    \n",
    "    # Transformamos en lista de tuplas para cumplir el tipo de dato de salida\n",
    "    tweets_data = [tuple(i) for i in tweets_data[[\"user\"]].itertuples()]\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Respuesta de la Q1 optimizando tiempo\n",
    "q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 s ± 3.76 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1449.68 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos como para nuestro q1 se obtuvo un tiempo de ejecución promedio de 18.2 segundos y un uso de memoria de alrededor de 1449.68 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el escenario de optimización de memoria utilizaremos el algoritmo de dividir y vencer. En el caso de la computación distruida como en clusteres spark como se mencionó antes se puede controlar esto a través del particionamiento de la inforación, esto permitirá a que cada nodo maneje un rango de datos mas pequeño, sin embargo para esto en fundamental el escalamiento horizontal, que en otras palabras significa agregar mas nodos al cluster de procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, y que de igual forma aplicaremos a las funciones q2 y q3 de optimización de memoria, será trabajar con archivos, que nos servirán a manera de \"nodos\" para hacer un procesamiento previo. Dicho de otra manera, ya que queremos obtener un resumen por dia del numero de tweets y por usuario, leeremos linea a linea el archivo de origin y lo iremos distribuyendo en varios archivos, uno por cada fecha a partir de los cuales calcularemos los valores deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path):\n",
    "    # Se leerá el archivo de datos origin y linea por linea se distibuye a un archivo de acuerdo a su fecha\n",
    "    # A su vez, contaremos el numero de lineas que tiene cada archivo para asi obterner el numero de tweets por cada dia\n",
    "    file_lines = {}\n",
    "\n",
    "    # Hacemos la lectura del archivo linea a linea\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_value = json.loads(line)\n",
    "            date = json_value.get(\"date\")[:10]\n",
    "            user = json_value.get(\"user\").get(\"username\")\n",
    "            data_id = json_value.get(\"id\")\n",
    "            data = {\"user\":user, \"id\":data_id}\n",
    "            with open(f\"data_q1/{date}\",\"a\") as fwrite:\n",
    "                fwrite.write(json.dumps(data)+\"\\n\") # Guardamos el registro en su archivo correspondiente\n",
    "                try:\n",
    "                    file_lines[date] += 1 # Contamos el numero de lineas en un diccionario por dia\n",
    "                except:\n",
    "                    file_lines[date] = 0 # Si no existe se empieza a contar\n",
    "    \n",
    "    # Convertimos en dataframe a nuestro diccionario y obtenemos el top 10 de dias con mas tweets\n",
    "    top_10_days = pd.DataFrame([{\"date\":i, \"rows\":file_lines[i]} for i in file_lines])\n",
    "    top_10_days = top_10_days.sort_values(\"rows\", ascending = False)[:10]\n",
    "    top_10_days = list(top_10_days['date'])\n",
    "    \n",
    "    # Tendremos asi particionados nuestros datos, leeremos ahora si con pandas read json y agrupamos por usuario para tomar el primero\n",
    "    result_list = []\n",
    "    # Notese que solo leemos los archivos de los top 10 de días\n",
    "    for i in top_10_days:\n",
    "        data_tmp = pd.read_json(f'data_q1/{i}', orient='records', lines=True)\n",
    "        data_tmp = data_tmp.groupby(\"user\").count().sort_values(\"id\", ascending = False)[:1]\n",
    "        user = data_tmp.index.values[0]\n",
    "        result_list.append((i, user))\n",
    "    \n",
    "    # Borramos los archivos auxiliares generados para alguna proxima ejecución\n",
    "    directory_path = 'data_q1'\n",
    "    file_list = os.listdir(directory_path)\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021-02-12', 'RanbirS00614606'),\n",
       " ('2021-02-13', 'MaanDee08215437'),\n",
       " ('2021-02-17', 'RaaJVinderkaur'),\n",
       " ('2021-02-16', 'jot__b'),\n",
       " ('2021-02-14', 'rebelpacifist'),\n",
       " ('2021-02-18', 'neetuanjle_nitu'),\n",
       " ('2021-02-15', 'jot__b'),\n",
       " ('2021-02-20', 'MangalJ23056160'),\n",
       " ('2021-02-23', 'Surrypuria'),\n",
       " ('2021-02-19', 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 3s ± 11.6 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 155.22 MiB, increment: 2.24 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenidos una vez los datos comprobamos como la ejecución q1_memory hizo un descenco significado\n",
    "\n",
    "| Funcion | Tiempo | Memoria |\n",
    "|---------|--------|---------|\n",
    "| q1_time | 18.2s | 1449.68 Mib |\n",
    "| q1_memory | 1min 3s | 155.22 MiB |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Los top 10 emojis más usados con su respectivo conteo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera para el conteo de emojis usaremos un enfoque similar, leeremos solos los campos necesarios en la lectura del archivo, en este caso el campo content, de cual obtendremos los emojis con la ayuda de la función analiza de la libreria emoji. Extraemos en una lista y la concatenamos en una lista global para todos el archivo para finalmente agruparlos usando un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # En este caso haremos la lectura igual que en la primera pregunta pero extraeremos unicamente el campo content\n",
    "    file_data = open(file_path, 'r')\n",
    "    Lines = file_data.readlines()\n",
    "    data = []\n",
    "\n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        data.append(json_value.get(\"content\"))\n",
    "\n",
    "    # Lista donde iremos concatenando los emojis de todo el archivo\n",
    "    emoji_values = []\n",
    "    \n",
    "    for i in data:\n",
    "        # Emoji analize retorna un tipo de dato con el valor del emoji y su posición, en este caso solo tomaremos el valor\n",
    "        emoji_values += [value.chars for value in emoji.analyze(i)]\n",
    "\n",
    "    #Vamos a crear un dataframe con los resultados\n",
    "    data = pd.DataFrame({\"emoji\":emoji_values})\n",
    "    data[\"counter\"] = 1\n",
    "    data = data.groupby('emoji').sum().sort_values(\"counter\", ascending = False).head(10)\n",
    "    emoji_list = [tuple(i) for i in data[[\"counter\"]].itertuples()]\n",
    "    file_data.close() #Liberamos memoria\n",
    "    \n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.5 s ± 2.83 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 597.83 MiB, increment: 16.88 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit resultado_q2_time = q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la ejecución con optimización de memoria nos apoyaremos de archivos en local para hacer un paso intermedio en el procesamiento, extraermos linea a linea los emojis de nuestra fuente de datos y los guardaremos en un archivo en local.\n",
    "\n",
    "Para el agrupamiento iremos leyendo linea a linea el archivo de preprocesamiento y los iremos agrupando en un diccionario de datos el cual procesaremos con pandas para obetener la respuesta deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Para evitar consumir la memoria en la lectura del archivo iremos leyendo linea a linea el archivo\n",
    "    # haremos una cuenta un agrupamiento previo e iremos almacenando en un archivo\n",
    "\n",
    "    # Lectura linea a linea\n",
    "    file1 = open(file_path,\"r\")\n",
    "    fwrite = open(\"aux_mem_q2/emoji_data\",\"a\")\n",
    "    file_read = open(\"aux_mem_q2/emoji_data\",\"r\")\n",
    "\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        # Leeremos solo el valor del content donde estan los emojis\n",
    "        content = json_value.get(\"content\")\n",
    "        \n",
    "        # Extraemos emojis y los agrupamos\n",
    "        tmp_emoji_list = [value.chars for value in emoji.analyze(content)]\n",
    "        tmp_emoji_list = '\\n'.join(tmp_emoji_list)\n",
    "        \n",
    "        # Si existen emojis en la linea los guardaremos en el archivo previo\n",
    "        if tmp_emoji_list:\n",
    "            fwrite.write(tmp_emoji_list+\"\\n\")\n",
    "\n",
    "    file1.close()\n",
    "    fwrite.close()\n",
    "        \n",
    "    # Ahora leeremos el archivo fila a fila y almacenaremos en un dictionario de datos sumando uno por cada ocurrencia\n",
    "    emoji_values = {}\n",
    "\n",
    "    for emoji_line in file_read:\n",
    "        if emoji_line.replace('\\n','') in emoji_values.keys():\n",
    "            emoji_values[emoji_line.replace('\\n','')] +=1 # Si existe se suma uno\n",
    "        else:\n",
    "            emoji_values[emoji_line.replace('\\n','')] = 0 # Si no existe lo crea\n",
    "    \n",
    "    #Lo hacemos dataframe para agrupar y sacar los maximos\n",
    "    emoji_values = pd.DataFrame({\"emoji\":list(emoji_values.keys()),\"conteo\":list(emoji_values.values())})\n",
    "    emoji_values = emoji_values.sort_values('conteo', ascending=False).head(10)\n",
    "\n",
    "    # #Lo volvemos listado de tuplas\n",
    "    emoji_values = [tuple(i) for i in emoji_values.set_index('emoji').itertuples()]\n",
    "\n",
    "    # Borrar el archivo generado\n",
    "    os.remove('aux_mem_q2/emoji_data')\n",
    "    file1.close()\n",
    "    \n",
    "    return emoji_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5048),\n",
       " ('😂', 3071),\n",
       " ('🚜', 2971),\n",
       " ('🌾', 2181),\n",
       " ('🇮🇳', 2085),\n",
       " ('🤣', 1667),\n",
       " ('✊', 1650),\n",
       " ('❤️', 1381),\n",
       " ('🙏🏻', 1316),\n",
       " ('💚', 1039)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.3 s ± 7.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 158.62 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera si hacemos una tabla comparativa podemos observar de mejor manera como efectivamente se aplico la optmización de memoria y tiempo respectivamente\n",
    "\n",
    "\n",
    "| Funcion | Tiempo | Memoria |\n",
    "|---------|--------|---------|\n",
    "| q2_time | 34.5s | 597.83.68 Mib |\n",
    "| q2_memory | 40.3s | 158.62 MiB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este problema, se tomará de igual manera los campos estrictamente necesarios, en este caso mentionedUsers el cual contiene un listado de usernames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # La documentacion no lo especifica pero en un analsis del archivo se puede observar que existe el campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    for line in Lines:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre así\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "    \n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.73 s ± 1.13 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 581.56 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la optimización de memoria de crearemos una lista donde se iran almancenando los valores linea a linea de las menciones de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # De igual manera se hara una lectura del archivo linea a linea para tomar los campos estrictamente necesarios\n",
    "    # Se toma el mismo campo mentionedUsers del cual se tomara los usernames\n",
    "    file1 = open(file_path, 'r')\n",
    "    list_mentioned_user = []\n",
    "    \n",
    "    #La lectura es linea a linea para no saturar memoria\n",
    "    for line in file1:\n",
    "        json_value = json.loads(line)\n",
    "        mentioned_users = json_value.get(\"mentionedUsers\")\n",
    "        if mentioned_users:\n",
    "            list_mentioned_user += [i[\"username\"] for i in mentioned_users] #Se tomara las n veces que se mecione en el tweet, se puede tomar unicos transformando a set y luego a list, pero en este caso lo mantedre así\n",
    "    \n",
    "    # Transformamos a dataframe para ordenar ocurrencias\n",
    "    mentioned_users = pd.DataFrame({\"mentioned_user\":list_mentioned_user})\n",
    "    mentioned_users[\"conteo\"] = 1\n",
    "    mentioned_users = mentioned_users.groupby(\"mentioned_user\").sum()\n",
    "    mentioned_users = mentioned_users.sort_values(\"conteo\", ascending = False).head(10)\n",
    "\n",
    "    #Transformamos a tuplas\n",
    "    mentioned_users = [tuple(i) for i in mentioned_users.itertuples()]\n",
    "\n",
    "    file1.close()\n",
    "    return mentioned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 s ± 960 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 150.88 MiB, increment: 6.93 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabla comparativa de tiempo de ejecución y uso de memoria por cada función\n",
    "\n",
    "\n",
    "| Funcion | Tiempo | Memoria |\n",
    "|---------|--------|---------|\n",
    "| q3_time | 6.73 | 581.56.68 Mib |\n",
    "| q3_memory | 11.1s  | 150.88 MiB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Se ha logrado hacer una optimización de las ejecuciones, al indentificar el cuello de botella que es la lectura y parseo de la información hemos podido atacar este paso en las ejecuciones dependiendo del tipo de optimización que se deseaba.\n",
    "\n",
    "Aunque nuestra implementación es una para una maquina unica, hemos aplicado algunos principios básicos que se hacen uso en la computación distribuida como es el particionamiento y el procesamiento en lotes.\n",
    "\n",
    "En caso de requerir procesar archivos mas grandes debemos hacer uso de cluster de procesamiento como Spark para distribuir de manera optima los datos, nuestros codigos pueden ser facilmente replanteados para DataFrames PySpark o usando Spark SQL para el procesamiento\n",
    "\n",
    "Me gustaría realizar una observación para el challenge, el esquema de la data presenta actualmente se encuentra desactualizada con respecto a la documentación oficial por lo que recomiendo la actualización de la data del challenge a uno con un esquema actual."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445f001600bf7eb0ddc22245e91c6123825f79238ec2a014331a6132a9c2d200"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
